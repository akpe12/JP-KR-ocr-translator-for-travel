{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "TrHlPFqwFAgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-jXeSJKE1WM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from typing import Dict, List\n",
        "import csv\n",
        "import torch\n",
        "from transformers import (\n",
        "    EncoderDecoderModel,\n",
        "    GPT2Tokenizer as BaseGPT2Tokenizer,\n",
        "    PreTrainedTokenizer, BertTokenizerFast,\n",
        "    PreTrainedTokenizerFast,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    XLMRobertaTokenizerFast,\n",
        "    Trainer\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.encoder_decoder.modeling_encoder_decoder import EncoderDecoderModel\n",
        "\n",
        "encoder_model_name = \"xlm-roberta-base\"\n",
        "decoder_model_name = \"skt/kogpt2-base-v2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "nEW5trBtbykK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Tokenizer(PreTrainedTokenizerFast):\n",
        "    def build_inputs_with_special_tokens(self, token_ids: List[int]) -> List[int]:\n",
        "        return token_ids + [self.eos_token_id]        \n",
        "\n",
        "src_tokenizer = XLMRobertaTokenizerFast.from_pretrained(encoder_model_name)\n",
        "trg_tokenizer = GPT2Tokenizer.from_pretrained(decoder_model_name, bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')"
      ],
      "metadata": {
        "id": "5ic7pUUBFU_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "DTf4U1fmFQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedDataset:\n",
        "    def __init__(self, \n",
        "        src_tokenizer: PreTrainedTokenizerFast, tgt_tokenizer: PreTrainedTokenizerFast,\n",
        "        file_path: str\n",
        "    ):\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = tgt_tokenizer\n",
        "        with open(file_path, 'r') as fd:\n",
        "            reader = csv.reader(fd)\n",
        "            next(reader)\n",
        "            self.data = [row for row in reader]\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
        "        src, trg = self.data[index]\n",
        "        embeddings = self.src_tokenizer(src, return_attention_mask=False, return_token_type_ids=False)\n",
        "        embeddings['labels'] = self.trg_tokenizer.build_inputs_with_special_tokens(self.trg_tokenizer(trg, return_attention_mask=False)['input_ids'])\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "train_dataset = PairedDataset(src_tokenizer, trg_tokenizer, '일-한 언어 corpus_train.csv')\n",
        "eval_dataset = PairedDataset(src_tokenizer, trg_tokenizer, '일-한 언어 corpus_eval.csv')        "
      ],
      "metadata": {
        "id": "65L4O1c5FLKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "uCBiLouSFiZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    encoder_model_name,\n",
        "    decoder_model_name,\n",
        "    pad_token_id=trg_tokenizer.bos_token_id,\n",
        ")\n",
        "model.config.decoder_start_token_id = trg_tokenizer.bos_token_id"
      ],
      "metadata": {
        "id": "I7uFbFYJFje8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for Trainer\n",
        "\n",
        "collate_fn = DataCollatorForSeq2Seq(src_tokenizer, model)\n",
        "wandb.init(project=\"temp\", name='roberta+kogpt2')\n",
        "\n",
        "arguments = Seq2SeqTrainingArguments(\n",
        "    output_dir='dump',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_total_limit=5,\n",
        "    dataloader_num_workers=1,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='wandb'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    arguments,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "YFq2GyOAUV0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "pPsjDHO5Vc3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoderModel.from_pretrained(\"best_model\")"
      ],
      "metadata": {
        "id": "_T4P4XunmK-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"dump/best_model\")"
      ],
      "metadata": {
        "id": "7vTqAgW6Ve3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 번역"
      ],
      "metadata": {
        "id": "r3DaD9wWG39T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"もんじゃ焼き\"\n",
        "embeddings = src_tokenizer(text, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt')\n",
        "embeddings = {k: v for k, v in embeddings.items()}\n",
        "output = model.generate(**embeddings)[0, 1:-1]"
      ],
      "metadata": {
        "id": "1vEI4JzwvP95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_tokenizer.decode(output.cpu())"
      ],
      "metadata": {
        "id": "tPon0BhJvwhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating model"
      ],
      "metadata": {
        "id": "ILiQFMswNbSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "9LXD0-DuNe9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "\n",
        "bleu = []\n",
        "f1 = []\n",
        "\n",
        "with torch.no_grad(), open('test.csv', 'r') as fd:\n",
        "    reader = csv.reader(fd)\n",
        "    next(reader)\n",
        "    datas = [row for row in reader]    \n",
        "\n",
        "    for data in tqdm(datas, \"Testing\"):\n",
        "        input, label = data\n",
        "        embeddings = src_tokenizer(input, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt')\n",
        "        embeddings = {k: v for k, v in embeddings.items()}\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**embeddings)[0, 1:-1]\n",
        "        preds = trg_tokenizer.decode(output.cpu())\n",
        "\n",
        "        bleu.append(sentence_bleu([label.split()], preds.split(), weights=[1,0,0,0]))"
      ],
      "metadata": {
        "id": "iCvhxb9ORQCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Bleu score: {mean(bleu)}\")"
      ],
      "metadata": {
        "id": "OfZRASeCbqec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
